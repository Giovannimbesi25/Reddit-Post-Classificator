{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13a7d0e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1> Reddit Classificator </h1>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865b5263",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<br>\n",
    "<center>\n",
    "<img src=\"images/startgame.jpg\" width=1000 height=500 /> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f858e87d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " <h2>  Abstract   </h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e662c0c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In questo tutorial si vuole costruire un sistema di classificazione real-time che permetta di suddividere i post di reddit in categorie, per poi visualizzarne graficamente i risultati. L'obbiettivo è quello di riuscire a predire l'argomento dei post, sulla base del solo titolo, evitando quindi che l'utente debba cliccarci."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004231cf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2> Data flow  </h2>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a901951",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<center>\n",
    "<img src=\"images/dataFlow.png\" width=1000 height=800 /> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d925b18",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <h2> Data Sources  </h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541511f3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Per ottenere i reddit post è stata utilizzata la libreria <span><a href=\"https://praw.readthedocs.io/en/stable/\">PRAW</a><span> di python.\n",
    "    <p></p>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474806cc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Una volta configurate le credenziali e scelto un subreddit, la libreria permette di ottenere varie informazioni quali il titolo, lo score, l'autore e così via. \n",
    "<p>\n",
    "Si è scelto di utilizzare uno script python tramite cui ottenere in streaming i post più recenti."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeb04f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>\n",
    "    \n",
    "Data Ingestion\n",
    "\n",
    "</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5508587",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "\n",
    "<img src=\"images/logst.png\" width=200 height=200 /> \n",
    "<p></p>\n",
    "Logstash è un progetto open-source per l'analisi dei log in tempo reale.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bd12a5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3>\n",
    "    How it works?\n",
    "</h3>\n",
    "<p>Si è utilizzato l'HTTP Input Plugin di Logstash, per ricevere i dati dallo script python</p>\n",
    "<center>\n",
    "<img src=\"images/patrick.png\" width=300 height=150 /> </center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a756630",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Attraverso il  Grok Filter Plugin, sono stati rimossi alcuni campi, superflui ai fini della classificazione, conservando unicamente il titolo dei post. Infine si è configurato l'output in modo tale da inviare i dati ad un cluster Kafka. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f7a5f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Kafka</h2>\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"images/kafka.png\" width=200 height=200 /> \n",
    "<p>Apache Kafka è una piattaforma per il data streaming distribuita che permette di pubblicare, sottoscrivere, archiviare ed elaborare flussi di record in tempo reale.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ac43de",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2>Usage</h2>\n",
    "<img src=\"images/kafkaMeme.png\" width=600 height=600 /> \n",
    "<p>Utilizzando Logstash come producer, egli invierà i dati al topic <b>reddit_post</b>: kafka si occuperà di conservare i dati e di gestirne la fruizione da parte dei consumer. </p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe7ebe8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>\n",
    "Data processing e machine learning\n",
    "<h2>\n",
    "<br>\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd4d866",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/spark.png\" width=200 height=200 /> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c960b040",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Il consumer è Spark, ovvero una piattaforma open source per l’elaborazione di analisi dei dati su larga scala, progettata per essere veloce e generica. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61e54a2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Per allenare il classificatore si è scelto di utilizzare il dataset fornito da  <span><a href=\"https://www.kaggle.com/datasets/mswarbrickjones/reddit-selfposts\">kaggle</a><span> contenente mille post per ognuna delle trenta classi identificate. Putroppo non è stato possibile selezionare tutte le classi di interesse, ma solo alcune, in quanto limitati dalla tecnologia... .\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"images/spark_model.png\" width=1000 height=600 /> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f18164",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Fondamentale è stato l'utilizzo della libreria  <span><a href=\"https://spark.apache.org/docs/latest/api/python/\">pyspark</a><span>, la quale permette di creare e salvare delle pipeline con cui performare il machine learning.\n",
    "Il modello prodotto ha ottenuto un'accuracy del 70% ed in generale delle buone performance.\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b99e0c6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"images/spark-accuracy.png\" width=400 height=700 /> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508dc5a3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ottenuto il modello si è passati all'utilizzo di Spark Structured Streaming, che ci permette di lavorare su dati in tempo reale, integrandosi perfettamente con Kafka.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94ba805",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Ogni dato ricevuto, viene passato al modello, il quale effettua la predizione.\n",
    "<br>\n",
    "\n",
    "I dati predetti vengono poi depositati su elasticsearch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc697fe",
   "metadata": {},
   "source": [
    "<img src=\"images/cert.png\" width=350 height=600 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e9fd1d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>\n",
    "Data Visualization\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08b5a5c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/kibana.png\" width=350 height=300 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7041c67c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Attraverso Kibana aggreghiamo i dati ottenuti da spark, e creiamo delle dashboard che ci permettono di visualizzare l'andamento dei dati, la predizione e altre informazioni, il tutto in tempo reale. Ecco alcuni esempi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0038817",
   "metadata": {},
   "source": [
    "Count of Category\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a804f9b5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<img src=\"images/kibana_1.png\" width=1500 height=1000 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92265311",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Most five popular words by Category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd345c59",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/kibana_2.png\" width=1500 height=1000 />\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
